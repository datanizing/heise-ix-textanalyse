{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantische Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kennst jetzt unterschiedliche Word Embeddings und kannst damit Ähnlichkeiten zwischen Wörtern selbst bestimmen.\n",
    "\n",
    "Allerdings gehen die Möglichkeiten der Embeddings noch weiter darüber hinaus. So kannst du eine Kombination von TF/IDF und Embeddings nutzen, um Texte in einen \"semantischen Raum\" zu transformieren und dort auch Ähnlichkeiten von Wörtern (oder Konzepten) ermitteln sowie semantische Suchen durchführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gewohnt lädst du die linguistisch analysierten Daten ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.db || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.db.gz && gunzip heise-articles-2020.db.gz\")\n",
    "    newsticker_db = 'heise-articles-2020.db'\n",
    "else:\n",
    "    newsticker_db = '../99_Common/heise-articles-2020.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import pandas as pd\n",
    "\n",
    "sql = sqlite3.connect(newsticker_db)\n",
    "df = pd.read_sql(\"SELECT * FROM nlp_articles WHERE datePublished<'2021-01-01' ORDER BY datePublished\", \n",
    "                 sql, index_col=\"id\", parse_dates=[\"datePublished\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vektorisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit du  häufige Wörter niedriger gewichten kannst, berechnest du zunächst die TF/IDF-Vektoren der Dokumente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as stop_words\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=10, sublinear_tf=True, use_idf=False)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df[\"nav\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wortvektoren berechnen (bzw. einladen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend lädst du die bereits berechneten Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ON_COLAB:\n",
    "    os.system(\"test -f heise-articles-2020.w2v || wget  https://datanizing.com/heiseacademy/nlp-course/blob/main/99_Common/heise-articles-2020.w2v.gz && gunzip heise-articles-2020.w2v.gz\")\n",
    "    w2v_file = \"heise-articles-2020.w2v\"\n",
    "else:\n",
    "    w2v_file = \"../99_Common/heise-articles-2020.w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"gensim>=4.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stattdessen könntest du auch ein fertiges Embedding nutzen, wenn deine Datenmenge zu klein ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantische Tranformation durchführen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion zur semantischen Transformation ist etwas komplizierter.\n",
    "\n",
    "* Du beginnst mit einer leeren Liste gemittelter Dokumentvektoren\n",
    "* Du iterierst zunächst über alle TF/IDF-Vektoren (`shape[0]` gibt dir die Anzahl der Zeilen), also über alle Dokumente\n",
    "* Du setzt den gemittelten Wortvektor des aktuellen Dokuments auf 0\n",
    "* Anschließend lässt du dir die Spalten ausgeben, die Werte != 0 haben (das entsprechende Wort kommt im Dokument vor)\n",
    "* Wenn sich das Wort auch im Embedding-Vokabular befindet, addierst du den (normierten!) Wortvektor gewichtet mit dem TF/IDF-Maß\n",
    "* Falls sich ein gemittelter Wortvektor ungleich des Nullvektors ergeben hat, wird er normiert\n",
    "* Anschließend fügst du den Wortvektor in die Liste der gemittelten Dokumentvektoren an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# averaged word vectors\n",
    "awv = []\n",
    "fn = tfidf_vectorizer.get_feature_names()\n",
    "for i in tqdm(range(tfidf_vectors.shape[0])):\n",
    "    v = np.zeros(w2v.vector_size)\n",
    "    # immer nur eine Zeile, rows wird nicht benötigt\n",
    "    rows,cols = tfidf_vectors[i].nonzero()\n",
    "    for c in cols:\n",
    "        feature = fn[c]\n",
    "        if feature in w2v.key_to_index :\n",
    "            # TF/IDF als Gewicht des normierten Wortvektors\n",
    "            wv = w2v[feature]\n",
    "            v += tfidf_vectors[i][0, c] * wv / np.linalg.norm(wv)\n",
    "    if np.linalg.norm(v) > 0:\n",
    "        v = v/np.linalg.norm(v)\n",
    "    awv.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kannst du Ähnlichkeiten berechnen, dazu nutzt du die dir schon bekannte Methode `cosine_similarity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möchtest du z.B. die Dokumente finden, die die größte *semantische Ähnlichkeit* zu `apple` haben, kannst du zunächst den Vektor bestimmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = w2v[\"apple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend berechnest du den Cosinus-Abstand zu allen Dokumenten (die Funktion erwartet zwei Matrizen, daher ist der zweite Vektor in `[]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cosine_similarity(awv, [apple])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dich interessiert nur das Dokument mit der größten Ähnlichkeit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[r.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[r.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spannend wird es jetzt, wenn du die Wortvektoren unterschiedlicher Wörter kombinierst. Da die Wortvektoren nicht normiert sind, musst du das noch nachholen, weil sonst verschiedene Wörter unterschiedlich stark beitragen würden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv =  w2v[\"google\"]/np.linalg.norm(w2v[\"google\"])\n",
    "wv += w2v[\"umsatz\"]/np.linalg.norm(w2v[\"umsatz\"])\n",
    "r = cosine_similarity(awv, [wv])\n",
    "df.iloc[r.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv =  w2v[\"google\"]/np.linalg.norm(w2v[\"google\"])\n",
    "wv += w2v[\"umsatz\"]/np.linalg.norm(w2v[\"umsatz\"])\n",
    "wv += w2v[\"microsoft\"]/np.linalg.norm(w2v[\"microsoft\"])\n",
    "r = cosine_similarity(awv, [wv])\n",
    "df.iloc[r.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[r.argmax()][\"full_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantische Methoden durch Kombination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast gesehen, wie sich Word Embeddings mit \"traditionellen\" Machine Learning-Methoden kombinieren lassen.\n",
    "\n",
    "Das Ergebnis ist durchaus beeindruckend. Besonders im letzten Fall wird eine Meldung gefunden, in der das Wort \"Umsatz\" gar nicht vorkommt, die sich aber dennoch damit beschäftigt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
